{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net layer dropout is 0.0\n",
      "Net layer dropout is 0.0\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_pre.data_loader import PropertyPredictionDataModule\n",
    "from data_pre.tokenizer import MolTranBertTokenizer\n",
    "from model.layers.main_layer import LightningModule\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "import matplotlib.pyplot as plt\n",
    "def prepare_data(model_name):\n",
    "\n",
    "     with open(f'Pretrained MoLFormer/hparams/{model_name}.yaml', 'r') as f:\n",
    "        config = Namespace(**yaml.safe_load(f))\n",
    "        \n",
    "        # prepare data:\n",
    "        data_module = PropertyPredictionDataModule(config)\n",
    "        data_module.prepare_data()\n",
    "\n",
    "        # data all loader\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        test_loader = data_module.val_dataloader()[1]\n",
    "\n",
    "        return test_loader,train_loader\n",
    "\n",
    "def prepare_model(model_name):\n",
    "        \n",
    "        with open(f'Pretrained MoLFormer/hparams/{model_name}.yaml', 'r') as f:\n",
    "        \n",
    "            config = Namespace(**yaml.safe_load(f))\n",
    "\n",
    "            tokenizer = MolTranBertTokenizer('bert_vocab.txt')\n",
    "\n",
    "            ckpt = f'Pretrained MoLFormer/checkpoints/{model_name}.ckpt'\n",
    "\n",
    "            model = LightningModule(config, tokenizer).load_from_checkpoint(ckpt, strict=False,config=config, tokenizer=tokenizer,vocab=len(tokenizer.vocab))\n",
    "\n",
    "            # Check for GPU availability\n",
    "            device = torch.device('cuda')\n",
    "            model = model.to(device)  # Move model to GPU if available\n",
    "            model.eval()\n",
    "            \n",
    "            return model\n",
    "        \n",
    "idx_size = 70\n",
    "mask_size = 70 + idx_size\n",
    "mz_size = 1 + mask_size\n",
    "adduct_size = 1 + mz_size\n",
    "ecfp_size =1024 + adduct_size\n",
    "\n",
    "model_name = 'XL_87'\n",
    "model = prepare_model(model_name)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict(data):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        device = 'cuda'\n",
    "        # 确保 data 是一个 numpy 数组，然后将其转化为 torch 张量\n",
    "        print(\"predict is running\")\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "\n",
    "        idx = data[:,:idx_size].long()\n",
    "        mask = data[:,idx_size:mask_size]\n",
    "        m_z = data[:,mask_size:mz_size].squeeze(-1)\n",
    "        adduct = data[:,mz_size:adduct_size].squeeze(-1).long()\n",
    "        ecfp = data[:,adduct_size:ecfp_size]\n",
    "        # idx, mask, m_z, adduct, ecfp,_ = [x.to(device) for x in data]\n",
    "\n",
    "        x = model.tok_emb(idx)\n",
    "        x = model.blocks(x)\n",
    "        # x = model.aggre(x, m_z, adduct, ecfp)\n",
    "\n",
    "        input_mask_expanded = mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        masked_embedding = x * input_mask_expanded\n",
    "        sum_embeddings = torch.sum(masked_embedding, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-8)\n",
    "        loss_input = sum_embeddings / sum_mask\n",
    "        loss_input = model.aggre(loss_input, m_z, adduct, ecfp)   \n",
    "        pred = model.net(loss_input)\n",
    "\n",
    "    return pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside prepare_dataset\n",
      "Length of dataset: 41816\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41816 entries, 0 to 41815\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Molecule Name  41816 non-null  object \n",
      " 1   CCS_AVG        41816 non-null  float64\n",
      " 2   Adduct         41816 non-null  object \n",
      " 3   Dimer.1        41816 non-null  object \n",
      " 4   inchi          41816 non-null  object \n",
      " 5   smiles         41816 non-null  object \n",
      " 6   m/z            41816 non-null  float64\n",
      " 7   ecfp           41816 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 2.9+ MB\n",
      "Length of dataset: 5804\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5804 entries, 0 to 5803\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Molecule Name  5804 non-null   object \n",
      " 1   CCS_AVG        5804 non-null   float64\n",
      " 2   Adduct         5804 non-null   object \n",
      " 3   Dimer.1        5804 non-null   object \n",
      " 4   inchi          5804 non-null   object \n",
      " 5   smiles         5804 non-null   object \n",
      " 6   m/z            5804 non-null   float64\n",
      " 7   ecfp           5804 non-null   object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 408.1+ KB\n",
      "Length of dataset: 11608\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11608 entries, 0 to 11607\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Molecule Name  11608 non-null  object \n",
      " 1   CCS_AVG        11608 non-null  float64\n",
      " 2   Adduct         11608 non-null  object \n",
      " 3   Dimer.1        11608 non-null  object \n",
      " 4   inchi          11608 non-null  object \n",
      " 5   smiles         11608 non-null  object \n",
      " 6   m/z            11608 non-null  float64\n",
      " 7   ecfp           11608 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 816.2+ KB\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m first_batch_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(first_batch_processed, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 按列（特征）进行堆叠\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# data = torch.tensor(first_batch_np, dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# explainer = shap.DeepExplainer( model, data[10:] )\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_batch_np\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 计算SHAP值\u001b[39;00m\n\u001b[1;32m     21\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(first_batch_np[\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/shap/explainers/_kernel.py:95\u001b[0m, in \u001b[0;36mKernel.__init__\u001b[0;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index_ordered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m model_null \u001b[38;5;241m=\u001b[39m match_model_to_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/test/lib/python3.8/site-packages/shap/utils/_legacy.py:199\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[0;34m(val, keep_index)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(val) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DenseData(val, [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)])\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(val))\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.core.series.Series\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DenseData(val\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(val))), \u001b[38;5;28mlist\u001b[39m(val\u001b[38;5;241m.\u001b[39mindex))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_dataloader,_ = prepare_data(model_name)\n",
    "    # c=\n",
    "    # device = 'cuda'\n",
    "    first_batch = next(iter(test_dataloader))\n",
    "\n",
    "    first_batch_processed = []\n",
    "    for x in first_batch:\n",
    "        if len(x.shape) == 1:  # 如果是一个一维张量，扩展成二维\n",
    "            x = x.unsqueeze(-1)\n",
    "        first_batch_processed.append(x.cpu().detach().numpy())\n",
    "    \n",
    "    # 将所有张量堆叠在一起，确保它们有相同的形状\n",
    "    first_batch_np = np.concatenate(first_batch_processed, axis=1)  # 按列（特征）进行堆叠\n",
    "\n",
    "    # data = torch.tensor(first_batch_np, dtype=torch.float32)\n",
    "    \n",
    "    # explainer = shap.DeepExplainer( model, data[10:] )\n",
    "    explainer = shap.KernelExplainer(model, first_batch_np[:2])\n",
    "    # 计算SHAP值\n",
    "    shap_values = explainer.shap_values(first_batch_np[3])\n",
    "\n",
    "    # init the JS visualization code\n",
    "    shap.initjs()\n",
    "\n",
    "    # plot the feature attributions\n",
    "    tokenizer = MolTranBertTokenizer('bert_vocab.txt')\n",
    "    \n",
    "    # Get SMILES and m/z (processed on CPU for compatibility with dataset)\n",
    "    smiles = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(first_batch_np[0,:70]))\n",
    "\n",
    "    lo = np.arange(1, 1168).reshape(-1, 1)  # 生成从 1 到 1167 的序列，并调整为列向量\n",
    "\n",
    "    print(shap_values[0].shape)\n",
    "    shap.save_html(\"force_plot.html\", shap.force_plot(explainer.expected_value[0], shap_values[0], first_batch_np[0] ,lo ,show=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
